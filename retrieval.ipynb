{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: intfloat/e5-large\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import weaviate\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from weaviate.classes.query import Filter\n",
    "from pymongo import MongoClient\n",
    "from langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from typing import Optional\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large\", cache_folder=\"./embedding_model\")\n",
    "MONGO_URI = \"mongodb://root:root@localhost:27017/\"\n",
    "DATABASE_NAME = \"incident_db\"\n",
    "COLLECTION_NAME = \"incident_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import root_validator\n",
    "\n",
    "class CustomReranker(BaseDocumentCompressor):\n",
    "    \"\"\"Document compressor using Flashrank interface.\"\"\"\n",
    "\n",
    "    client: Ranker\n",
    "    \"\"\"Flashrank client to use for compressing documents\"\"\"\n",
    "    top_n: int = 3\n",
    "    \"\"\"Number of documents to return.\"\"\"\n",
    "    model: Optional[str] = None\n",
    "    \"\"\"Model to use for reranking.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        extra = 'forbid'\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def validate_environment(cls, values):\n",
    "        \"\"\"Validate that api key and python package exists in environment.\"\"\"\n",
    "        try:\n",
    "            from flashrank import Ranker\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Could not import flashrank python package. \"\n",
    "                \"Please install it with `pip install flashrank`.\"\n",
    "            )\n",
    "\n",
    "        values[\"model\"] = values.get(\"model\", \"ms-marco-MiniLM-L-12-v2\")\n",
    "        values[\"client\"] = Ranker(model_name=values[\"model\"], cache_dir=\"reranker\")\n",
    "        return values\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents,\n",
    "        query,\n",
    "        callbacks = None):\n",
    "        passages = [\n",
    "            {\"id\": i, \"text\": doc.page_content, \"metadata\": doc.metadata} for i, doc in enumerate(documents)\n",
    "        ]\n",
    "        rerank_request = RerankRequest(query=query, passages=passages)\n",
    "        rerank_response = self.client.rerank(rerank_request)[:self.top_n]\n",
    "        final_results = []\n",
    "        for r in rerank_response:\n",
    "            doc = Document(\n",
    "                page_content=r[\"text\"],\n",
    "                metadata={\n",
    "                    **r['metadata'],\n",
    "                    \"id\": r[\"id\"],\n",
    "                    \"relevance_score\": r[\"score\"]\n",
    "                },\n",
    "            )\n",
    "            final_results.append(doc)\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = CustomReranker()\n",
    "\n",
    "def create_retriever(industries):\n",
    "    filters = None\n",
    "    if not industries == 'all':\n",
    "        filters = Filter.any_of([Filter.by_property(\"industry\").equal(industry) for industry in industries])\n",
    "    db = WeaviateVectorStore(client=weaviate_client, index_name=\"incident\", text_key=\"text\", embedding=embeddings)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor = compressor,\n",
    "        base_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"fetch_k\": 20, 'filters': filters})\n",
    "    )\n",
    "    return compression_retriever\n",
    "\n",
    "def get_documents_ids(retrieved_docs):\n",
    "    if retrieved_docs:\n",
    "        return [int(doc.metadata['incident_id']) for doc in retrieved_docs]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_documents_by_ids(ids):\n",
    "    try:\n",
    "        client = MongoClient(MONGO_URI)\n",
    "        db = client[DATABASE_NAME]\n",
    "        collection = db[COLLECTION_NAME]        \n",
    "        documents = list(collection.find({\"accident_id\": {\"$in\": ids}}))\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        return []\n",
    "    finally:\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# industries = ['processing of metals']\n",
    "# filters = Filter.all_of([Filter.by_property(\"industry\").equal(industry) for industry in industries])\n",
    "# db = WeaviateVectorStore(client=weaviate_client, index_name=\"incident\", text_key=\"text\", embedding=embeddings)\n",
    "# compression_retriever = ContextualCompressionRetriever(\n",
    "#     base_compressor = compressor,\n",
    "#     base_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"fetch_k\": 20, 'filters': filters})\n",
    "# )\n",
    "\n",
    "# compression_retriever.invoke('what are probable causes of a fire in an industrial plant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.runnables import RunnableBranch, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  openai_api_base=\"https://api.groq.com/openai/v1/\",\n",
    "  model = \"llama-3.3-70b-versatile\",\n",
    "  temperature=0.7,\n",
    "  api_key=\"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\"\n",
    ")\n",
    "\n",
    "def format_chat_history(history):\n",
    "  formatized_chat_history = \"\"\n",
    "  for message in history:\n",
    "    message_type = str(type(message)).split(\"'\")[1].split(\".\")[-1]\n",
    "    message_content = message.content.replace(\"\\n\", \"\")\n",
    "    formatized_chat_history += f\"\\t{message_type}: {message_content}\\n\"\n",
    "  return formatized_chat_history\n",
    "\n",
    "CONTEXT_TEMPLATE = \"\"\"\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "Given a discussion history and a follow-up question, rewrite the follow-up question to be fully self-contained and understandable without the context of the previous conversation. Keep it as close as possible to the original meaning but include any relevant details from the history if they add clarity or context. If no additional context is needed, leave the question unchanged.\n",
    "Discussion history:{chat_history}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user|end_header_id|>\n",
    "Question: {question}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "CONTEXT_PROMPT = ChatPromptTemplate.from_template(CONTEXT_TEMPLATE)\n",
    "\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are IncidentNavigator, an AI designed to assist in managing and understanding incidents using a dataset of incident records. Your role is to provide precise, concise, and clear responses based on the context of the documents you receive. If a question falls outside of the information available in the provided context, you should clearly state that you cannot provide an answer but will offer the best response based on what is available.\n",
    "The documents you process include the following fields:\n",
    "- accident_id: Unique identifier for each incident.\n",
    "- event_type: Category of the incident (e.g., fire, collision).\n",
    "- industry_type: The sector or industry where the incident occurred (e.g., construction, transportation).\n",
    "- accident_title: A brief, descriptive title for the accident.\n",
    "- start_date: The date and time the incident began.\n",
    "- finish_date: The date and time the incident ended or was resolved.\n",
    "- accident_description: A detailed account of how the accident occurred.\n",
    "- causes_of_accident: Factors or conditions leading to the incident.\n",
    "- consequences: Outcomes or impacts of the incident (e.g., injuries, damage).\n",
    "- emergency_response: Immediate actions taken to manage the incident.\n",
    "- lesson_learned: Insights or recommendations for future prevention.\n",
    "- url: Reference link to the document webpage.\n",
    "When answering questions, follow these guidelines:\n",
    "- Context Provided: If the context includes information related to these fields, provide a direct and detailed response based on the relevant data.\n",
    "- Context Missing or Insufficient: If no context or relevant information is provided:\n",
    "  - State that you cannot provide a definitive answer because the requester does not have sufficient privileges or the information is unavailable.\n",
    "  - Do not speculate but offer a general response or guidance based on the type of question, when possible.\n",
    "Context: {context}\n",
    "You must only output JSON conforming to the schema below: \n",
    "\"properties\": {{\n",
    "  \"answer\": {{\n",
    "    \"description\": \"Detailed answer or response based on the provided context or available data\",\n",
    "    \"type\": \"string\",\n",
    "    \"required\": true\n",
    "  }},\n",
    "  \"references\": {{\n",
    "    \"description\": \"References to context documents\",\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {{\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {{\n",
    "        \"accident_id\": {{ \"type\": \"string\" }},\n",
    "        \"event_type\": {{ \"type\": \"string\" }},\n",
    "        \"industry_type\": {{ \"type\": \"string\" }},\n",
    "        \"accident_title\": {{ \"type\": \"string\" }},\n",
    "        \"start_date\": {{ \"type\": \"string\" }},\n",
    "        \"finish_date\": {{ \"type\": \"string\" }},\n",
    "        \"url\": {{ \"type\": \"string\" }}\n",
    "      }}\n",
    "    }},\n",
    "    \"required\": true\n",
    "  }}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user|end_header_id|>\n",
    "Question: {question}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = ChatPromptTemplate.from_template(SYSTEM_TEMPLATE)\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "  def default(self, obj):\n",
    "      if isinstance(obj, datetime):\n",
    "          return obj.isoformat()\n",
    "      return super().default(obj)\n",
    "  \n",
    "def retrieve(data):\n",
    "  industries = data['industries']\n",
    "  query = data['question']\n",
    "  retriever = create_retriever(industries)\n",
    "  docs = retriever.invoke(query)\n",
    "  ids = get_documents_ids(docs)\n",
    "  retrieved_docs = get_documents_by_ids(ids)\n",
    "  for document in retrieved_docs:\n",
    "      document.pop(\"_id\", None)\n",
    "  data['context'] = \"\\n\\n\".join(json.dumps(document, cls=CustomJSONEncoder) for document in retrieved_docs)\n",
    "  data.pop(\"industries\")\n",
    "  return data\n",
    "\n",
    "def get_industry(placeholder = None):\n",
    "  return ['processing of metals', 'power generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8080/v1/schema/Incident \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8080/v1/schema/Incident \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def get_json_from_string(json_string):\n",
    "    try:\n",
    "        json_data = json.loads(json_string)\n",
    "        return json_data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "memory = ConversationBufferWindowMemory()\n",
    "\n",
    "runnable_context = RunnablePassthrough.assign(\n",
    "    chat_history = RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\") | format_chat_history\n",
    ") | CONTEXT_PROMPT | llm | {'question': StrOutputParser()}\n",
    "\n",
    "runnable_system = RunnablePassthrough.assign(\n",
    "    industries = RunnableLambda(get_industry)\n",
    ") | retrieve\n",
    "\n",
    "rag_chain = runnable_context | runnable_system | SYSTEM_PROMPT | llm | StrOutputParser() | get_json_from_string\n",
    "\n",
    "rag_chain.invoke({\"question\": \"what are ways to prevent a fire in an industrial plant\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\n",
    "#     \"explodinggradients/amnesty_qa\",\n",
    "#     \"english_v3\",\n",
    "#     trust_remote_code=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"eval\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "# from ragas import evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
