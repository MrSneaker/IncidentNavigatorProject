{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import weaviate\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from weaviate.classes.query import Filter\n",
    "from pymongo import MongoClient\n",
    "from html import unescape\n",
    "from langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from typing import Optional\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large\", cache_folder=\"./embedding_model\")\n",
    "MONGO_URI = \"mongodb://root:root@localhost:27017/\"\n",
    "DATABASE_NAME = \"incident_db\"\n",
    "COLLECTION_NAME = \"incident_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import root_validator\n",
    "\n",
    "class CustomReranker(BaseDocumentCompressor):\n",
    "    \"\"\"Document compressor using Flashrank interface.\"\"\"\n",
    "\n",
    "    client: Ranker\n",
    "    \"\"\"Flashrank client to use for compressing documents\"\"\"\n",
    "    top_n: int = 3\n",
    "    \"\"\"Number of documents to return.\"\"\"\n",
    "    model: Optional[str] = None\n",
    "    \"\"\"Model to use for reranking.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        extra = 'forbid'\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def validate_environment(cls, values):\n",
    "        \"\"\"Validate that api key and python package exists in environment.\"\"\"\n",
    "        try:\n",
    "            from flashrank import Ranker\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Could not import flashrank python package. \"\n",
    "                \"Please install it with `pip install flashrank`.\"\n",
    "            )\n",
    "\n",
    "        values[\"model\"] = values.get(\"model\", \"ms-marco-MiniLM-L-12-v2\")\n",
    "        values[\"client\"] = Ranker(model_name=values[\"model\"], cache_dir=\"reranker\")\n",
    "        return values\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents,\n",
    "        query,\n",
    "        callbacks = None):\n",
    "        passages = [\n",
    "            {\"id\": i, \"text\": doc.page_content, \"metadata\": doc.metadata} for i, doc in enumerate(documents)\n",
    "        ]\n",
    "        rerank_request = RerankRequest(query=query, passages=passages)\n",
    "        rerank_response = self.client.rerank(rerank_request)[:self.top_n]\n",
    "        final_results = []\n",
    "        for r in rerank_response:\n",
    "            doc = Document(\n",
    "                page_content=r[\"text\"],\n",
    "                metadata={\n",
    "                    **r['metadata'],\n",
    "                    \"id\": r[\"id\"],\n",
    "                    \"relevance_score\": r[\"score\"]\n",
    "                },\n",
    "            )\n",
    "            final_results.append(doc)\n",
    "        return final_results\n",
    "    \n",
    "compressor = CustomReranker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(user_industry):\n",
    "    industry_filter = Filter.by_property(\"industry\").equal(user_industry)\n",
    "    db = WeaviateVectorStore(client=weaviate_client, index_name=\"incident\", text_key=\"text\", embedding=embeddings)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor = compressor,\n",
    "        base_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"fetch_k\": 20, 'filters': industry_filter})\n",
    "    )\n",
    "    return compression_retriever\n",
    "\n",
    "def get_documents_ids(retrieved_docs):\n",
    "    if retrieved_docs:\n",
    "        return [int(doc.metadata['incident_id']) for doc in retrieved_docs]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_documents_by_ids(ids):\n",
    "    try:\n",
    "        client = MongoClient(MONGO_URI)\n",
    "        db = client[DATABASE_NAME]\n",
    "        collection = db[COLLECTION_NAME]        \n",
    "        documents = list(collection.find({\"accident_id\": {\"$in\": ids}}))\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        return []\n",
    "    finally:\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Chatbot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from IPython.display import Image, display\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  openai_api_base=\"https://api.groq.com/openai/v1/\",\n",
    "  model = \"llama-3.3-70b-specdec\",\n",
    "  temperature=0.7,\n",
    "  api_key=\"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\"\n",
    ")\n",
    "\n",
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        return super().default(obj)\n",
    "\n",
    "def format_chat_history(data):\n",
    "  formatize_chat_history = \"\"\n",
    "  if \"chat_history\" in data.keys():\n",
    "    for message in data[\"chat_history\"]:\n",
    "      message_type = str(type(message)).split(\"'\")[1].split(\".\")[-1]\n",
    "      message_content = message.content.replace(\"\\n\", \"\")\n",
    "      formatize_chat_history += f\"\\t{message_type}: {message_content}\\n\"\n",
    "    data[\"chat_history\"] = formatize_chat_history\n",
    "  return data\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "def retrieve(industry, query):\n",
    "    retriever = create_retriever(industry)\n",
    "    docs = retriever.invoke(query)\n",
    "    ids = get_documents_ids(docs)\n",
    "    retrieved_docs = get_documents_by_ids(ids)\n",
    "    for document in retrieved_docs:\n",
    "        document.pop(\"_id\", None) \n",
    "    return \"\\n\\n\".join(json.dumps(document, cls=CustomJSONEncoder) for document in retrieved_docs), retrieved_docs\n",
    "\n",
    "CONDENSE_QUESTION_TEMPLATE = \"\"\"\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "Given a discussion history and a follow-up question, rewrite the follow-up question to be fully self-contained and understandable without the context of the previous conversation. Keep it as close as possible to the original meaning but include any relevant details from the history if they add clarity or context. If no additional context is needed, leave the question unchanged.\n",
    "Discussion history:{chat_history}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user|end_header_id|>\n",
    "Question: {question}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "FINAL_ANSWER_PROMPT_TEMPLATE = \"\"\"\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are IncidentNavigator, an AI designed to assist in managing and understanding incidents using a dataset of incident records. Your role is to provide precise, concise, and clear responses based on the context of the documents you receive. If a question falls outside of the information available in the provided context, you should clearly state that you cannot provide an answer but will offer the best response based on what is available.\n",
    "The documents you process include the following fields:\n",
    "- accident_id: Unique identifier for each incident.\n",
    "- event_type: Category of the incident (e.g., fire, collision).\n",
    "- industry_type: The sector or industry where the incident occurred (e.g., construction, transportation).\n",
    "- accident_title: A brief, descriptive title for the accident.\n",
    "- start_date: The date and time the incident began.\n",
    "- finish_date: The date and time the incident ended or was resolved.\n",
    "- accident_description: A detailed account of how the accident occurred.\n",
    "- causes_of_accident: Factors or conditions leading to the incident.\n",
    "- consequences: Outcomes or impacts of the incident (e.g., injuries, damage).\n",
    "- emergency_response: Immediate actions taken to manage the incident.\n",
    "- lesson_learned: Insights or recommendations for future prevention.\n",
    "When answering questions, provide a direct response based on these fields. If the context is insufficient or unclear, state that you cannot provide a definitive answer and offer a general response based on available information.\n",
    "Context: {context}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user|end_header_id|>\n",
    "Question: {question}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
